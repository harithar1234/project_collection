{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-MNIST-Handwritten_digits.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset.The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "\n"
      ],
      "metadata": {
        "id": "7mrMl2NtR93P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "71V_UsoSKiqA"
      },
      "outputs": [],
      "source": [
        "#necessary libraries\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.special import softmax\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ReLu function\n",
        "def relu(X):\n",
        "   return np.maximum(0,X)\n",
        "\n",
        "#softmax function\n",
        "def soft_max(X):\n",
        "    m = softmax(X, axis=1)\n",
        "    return m\n",
        "\n",
        "#relu derivative\n",
        "def Relu_deriv(x):\n",
        "        o=relu(x)\n",
        "        o[o>0]=1\n",
        "        return o"
      ],
      "metadata": {
        "id": "Mx7MAHvpLhKg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{width input - kernel size + 2* padding}{stride} + 1 = width output $\n",
        "\n",
        "we require width input = width output ,so padding=\n",
        "\n",
        "$padding = \\frac{stride(width input-1) + kernel - width input}{2}$"
      ],
      "metadata": {
        "id": "kYngOYVfP9Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def small_conv(slice,filter):\n",
        "    # Element-wise product of slice and filter. \n",
        "    prod = np.multiply(slice,filter)\n",
        "    # Sum over all entries of the volume prod.\n",
        "    out = np.sum(prod)\n",
        "    return out"
      ],
      "metadata": {
        "id": "jNA0XPAsX9vt"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implements the forward propagation for a convolution function\n",
        "\n",
        "def conv_forward(Activation_prev, kernel ):\n",
        "\n",
        "# Activation_prev = output of the previous layer (input to the convolution layer)\n",
        "  \n",
        "    (num_datapoints, W_prev, W_prev, num_Channels) = Activation_prev.shape\n",
        "    \n",
        "    (k, k, num_Channels, n_kernels) = kernel.shape\n",
        "    \n",
        "    stride = 1 #given \n",
        "    pad = 2 #calculated\n",
        "    new_W = int((W_prev + 2*pad - k)/stride) + 1     # dimension of the CONVOLUTION LAYER output \n",
        "    \n",
        "    # Initialize the volume Z with zeros.\n",
        "    Z = np.zeros([num_datapoints, new_W, new_W, n_kernels])\n",
        "    \n",
        "    Activation_padded = np.pad(Activation_prev, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
        "\n",
        "    \n",
        "    for i in range(num_datapoints):                              \n",
        "        x_pad = Activation_padded[i,:,:,:]   # padded activation of ith training sample\n",
        "        for h in range(new_W):                          \n",
        "            for w in range(new_W):                     \n",
        "                for c in range(n_kernels):                 \n",
        "                    \n",
        "                    # corners of the current slice\n",
        "                    ver_start = h*stride\n",
        "                    ver_end = h*stride + k\n",
        "                    hor_start = w*stride \n",
        "                    hor_end = w*stride + k\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of x_pad \n",
        "                    x_slice = x_pad[ver_start:ver_end,hor_start:hor_end,:]\n",
        "                    # Convolve the (3D) slice with the respective filter \n",
        "                    Z[i, h, w, c] = small_conv(x_slice, kernel[:, :, :, c])\n",
        "                                        \n",
        "    \n",
        "    conv_output=relu(Z)\n",
        "    # info =needed information for the conv_backward() function\n",
        "    info= (Activation_prev, kernel ,Z)\n",
        "    return conv_output,info"
      ],
      "metadata": {
        "id": "NWEBc2rTLj23"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implements the backward propagation for a convolution function\n",
        "\n",
        "def conv_backward(dC, infor):\n",
        "    \n",
        "#  dC = gradient of the error with respect to the output of the conv layer {array of shape (num_datapoints, new_W, new_W, n_kernels)}\n",
        "#  infor= needed values for the conv_backward() got from conv_forward()\n",
        "#  A_prev = input given to the convolution layer\n",
        "# Z= convolution of A_prev and kernel (without relu function applied)\n",
        "    \n",
        "    (A_prev, kernel, Z) = infor  \n",
        "\n",
        "    dC_wrt_dZ=Relu_deriv(Z)\n",
        "\n",
        "    dZ= np.multiply( dC , dC_wrt_dZ)\n",
        "# dZ= gradient of the error with respect to Z , {array of shape (num_datapoints, new_W, new_W, n_kernels)}\n",
        "    \n",
        "    (num_datapoints, W_prev, W_prev, num_Channels) = A_prev.shape\n",
        "    (k, k, num_Channels, n_kernels) = kernel.shape\n",
        "\n",
        "    stride = 1 # given\n",
        "    pad = 2 # calculated\n",
        "    \n",
        "    (num_datapoints, new_W, new_W, n_kernels) = dZ.shape\n",
        "\n",
        "# dA_prev= gradient of the error with respect to A_prev(input of convolution layer)    \n",
        "# dker = gradient of the error with respect to kernel of convolution layer\n",
        "    # Initialize dA_prev, dKer\n",
        "    dA_prev = np.zeros((num_datapoints, W_prev, W_prev, num_Channels))                           \n",
        "    dker = np.zeros((k, k, num_Channels, n_kernels))\n",
        "  \n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_padded = np.pad(A_prev, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
        "    dA_prev_padded =np.pad(dA_prev, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
        " \n",
        "    for i in range(num_datapoints):                 \n",
        "        \n",
        "        x_prev_pad = A_prev_padded[i]         #  ith training example from A_prev_pad \n",
        "        dx_prev_pad = dA_prev_padded[i]      #  ith training example from dA_prev_pad\n",
        "        \n",
        "        for h in range(new_W):                  \n",
        "            for w in range(new_W):              \n",
        "                for c in range(n_kernels):         \n",
        "                    \n",
        "                    # corners of the current slice\n",
        "                    ver_start = h\n",
        "                    ver_end = ver_start + k\n",
        "                    hor_start = w\n",
        "                    hor_end = hor_start + k\n",
        "                    \n",
        "                    # Use the corners to define the slice from x_prev_pad\n",
        "                    x_slice = x_prev_pad[ver_start:ver_end, hor_start:hor_end, :]\n",
        "                    \n",
        "                    dx_prev_pad[ver_start:ver_end, hor_start:hor_end, :] += kernel[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dker[:,:,:,c] += x_slice * dZ[i, h, w, c]\n",
        "              \n",
        "        # unpadding dx_prev_pad to get dA_prev of ith example\n",
        "        dA_prev[i, :, :, :] = dx_prev_pad[pad:-pad, pad:-pad, :]\n",
        "  \n",
        "    return dA_prev, dker\n"
      ],
      "metadata": {
        "id": "Dazg4FdOcbr4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implements the forward pass of the pooling layer\n",
        "\n",
        "def pool_forward(Activation_prev ):\n",
        "# Activation_prev= input of pooling layer (output of previous layer)\n",
        "\n",
        "    (num_datapoints, W_prev, W_prev, num_Channels) = Activation_prev.shape\n",
        "    pool_size = 2 # given\n",
        "    stride = 2 #given\n",
        "    \n",
        "    # Define the dimensions of the output\n",
        "    new_W = int(1 + (W_prev - pool_size) / stride)\n",
        "    # Initialize output matrix \n",
        "    out_pool = np.zeros((num_datapoints, new_W, new_W, num_Channels))              \n",
        "    \n",
        "    for i in range(num_datapoints):                         \n",
        "        for h in range(new_W):                     \n",
        "            for w in range(new_W):                \n",
        "                for c in range (num_Channels):            \n",
        "                    \n",
        "                    #corners of the current slice\n",
        "                    ver_start = h*stride\n",
        "                    ver_end = h*stride + pool_size\n",
        "                    hor_start = w*stride\n",
        "                    hor_end = w*stride + pool_size\n",
        "                    \n",
        "                    # Use the corners to define the current slice of the ith training example Activation_prev\n",
        "                    a_slice = Activation_prev[i, ver_start:ver_end, hor_start:hor_end,c]\n",
        "                    \n",
        "                    # pooling operation on the slice\n",
        "                    out_pool[i, h, w, c] = np.max(a_slice)\n",
        "                            \n",
        "    return out_pool"
      ],
      "metadata": {
        "id": "4Yz-2h-9xeAQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_mask(x):\n",
        "#mask contains a True at the position corresponding to the max entry of x and contains false at other positions\n",
        "    mask = x == np.max(x)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "NGX-f_1t3lAo"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Implements the backward pass of the pooling layer\n",
        "def pool_backward(dPool, A_prev):\n",
        "  \n",
        "#dPool = gradient of error with respect to the output of the pooling layer\n",
        "# A_prev = input of pooling layer      \n",
        "#dA_prev = gradient of error with respect to the input of the pooling layer (same shape as A_prev)\n",
        "    stride = 2 # given\n",
        "    pool_size =2 #given\n",
        "    \n",
        "    num_datapoints, new_W, new_W, num_Channels = dPool.shape\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "    \n",
        "    for i in range(num_datapoints):                     \n",
        "        x = A_prev[i]          # ith training example from A_prev\n",
        "        for h in range(new_W):                  \n",
        "            for w in range(new_W):              \n",
        "                for c in range(num_Channels):          \n",
        "                    #corners of the current slice (array 2D)\n",
        "                    ver_start = h\n",
        "                    ver_end = ver_start + pool_size\n",
        "                    hor_start = w\n",
        "                    hor_end = hor_start + pool_size\n",
        "                    \n",
        "                    # Use the corners and c to define the current slice (array 2D) from x\n",
        "                    x_slice = x[ver_start:ver_end, hor_start:hor_end, c]\n",
        "                    # Create the mask from x_slice\n",
        "                    mask = max_mask(x_slice)\n",
        "                    # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dPool) \n",
        "                    dA_prev[i, ver_start:ver_end, hor_start:hor_end, c] += np.multiply(mask, dPool[i, h, w, c])\n",
        "          \n",
        "    return dA_prev"
      ],
      "metadata": {
        "id": "cQ3h6tOP8hAW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flattening (unraveling) layer\n",
        "def flat_forward( Prev_Act ):\n",
        "# Prev_Act = output of previous pooling layer , (input to the  flattening layer)\n",
        "   num_datapoints, new_W, new_W, num_Channels = Prev_Act.shape\n",
        "   flatten_output= np.zeros((num_datapoints, (new_W*new_W*num_Channels)))\n",
        "   \n",
        "   for i in range(num_datapoints):                     \n",
        "        act_map_volume = Prev_Act[i]          # ith training example from Prev_Act\n",
        "        feature_vec=act_map_volume.flatten()\n",
        "        flatten_output[i,:]=feature_vec\n",
        "   \n",
        "   return flatten_output"
      ],
      "metadata": {
        "id": "SER1rP2iCgoR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_backpropagation(dF, Prev_Act):\n",
        "   # Prev_Act = output of previous pooling layer , (input to the  flattening layer)\n",
        "   # dF = gradient of error with respect to output of flattening layer\n",
        "   # dPrev_Act = gradient of error with respect to input of flattening layer,(same shape as Prev_Act)\n",
        "   dPrev_Act=dF.reshape(Prev_Act.shape)\n",
        "   return dPrev_Act"
      ],
      "metadata": {
        "id": "hCnwL7Vagiaz"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_forward(feature_vec,W_hidden,b_hidden,W_out,b_out):\n",
        "#feature_vec = output of flatten layer.(input to MLP)    \n",
        "    Z_h = np.dot(feature_vec,W_hidden) + b_hidden\n",
        "    activation_hidden = relu(Z_h)\n",
        "\n",
        "    # Phase 2\n",
        "    Z_o = np.dot(activation_hidden, W_out) + b_out\n",
        "    y_pred = soft_max(Z_o)\n",
        "    \n",
        "    #information needed for backpropagation of MLP\n",
        "    info_MLP=(feature_vec,W_hidden,b_hidden,Z_h,activation_hidden,W_out,b_out,Z_o,y_pred)\n",
        "    return y_pred,info_MLP"
      ],
      "metadata": {
        "id": "kQeaA5tr7SAH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_backward(y_label,info_MLP):\n",
        "       #information for backpropagation of MLP got from MLP_forward()\n",
        "       (feature_vec,W_hidden,b_hidden,Z_h,activation_hidden,W_out,b_out,Z_o,y_pred)=info_MLP\n",
        "       \n",
        "       dZ_o = y_pred - y_label\n",
        "       dZ_o_wrt_W_out = activation_hidden\n",
        "\n",
        "       dW_out = np.dot(dZ_o_wrt_W_out.T, dZ_o)\n",
        "       db_out = dZ_o     \n",
        "\n",
        "       dZ_o_wrt_act_hidden = W_out\n",
        "       dactivation_hidden = np.dot(dZ_o , dZ_o_wrt_act_hidden.T)\n",
        "       dact_hidden_wrt_Z_h = Relu_deriv(Z_h)\n",
        "       dZ_h_wrt_W_hidden = feature_vec\n",
        "       dW_hidden = np.dot(dZ_h_wrt_W_hidden.T, dact_hidden_wrt_Z_h * dactivation_hidden)\n",
        "       db_hidden = dactivation_hidden * dact_hidden_wrt_Z_h    \n",
        "\n",
        "       dZ_h_wrt_input= W_hidden   \n",
        "       dinput=np.dot(dact_hidden_wrt_Z_h * dactivation_hidden , dZ_h_wrt_input.T)\n",
        "\n",
        "       return dinput,dW_out,db_out,dW_hidden,db_hidden\n"
      ],
      "metadata": {
        "id": "E5aZka4ziWGC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Vanilla_SGD(gradients,parameters):\n",
        "      learn_rate=0.001\n",
        "      (dkernel1 , dkernel2, dW_hidden , db_hidden , dW_out , db_out)=gradients\n",
        "      (kernel1_old , kernel2_old , W_hidden_old , b_hidden_old, W_out_old , b_out_old )=parameters\n",
        "      \n",
        "      kernel1_new = kernel1_old - learn_rate* (dkernel1) \n",
        "      kernel2_new = kernel2_old - learn_rate* (dkernel2) \n",
        "      \n",
        "      W_hidden_new = W_hidden_old - learn_rate* (dW_hidden) \n",
        "      b_hidden_new = b_hidden_old - learn_rate* (db_hidden) \n",
        "      \n",
        "      W_out_new = W_out_old - learn_rate* (dW_out) \n",
        "      b_out_new = b_out_old - learn_rate* (db_out) \n",
        "\n",
        "      parameters=(kernel1_new ,kernel2_new ,W_hidden_new ,b_hidden_new , W_out_new , b_out_new)\n",
        "        \n",
        "      return parameters\n",
        "      "
      ],
      "metadata": {
        "id": "hZ3uLi_k5MXt"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def  Momentum(gradients,parameters,V):\n",
        "          learn_rate=0.001\n",
        "          alpha = 0.9\n",
        "          (dkernel1 , dkernel2, dW_hidden , db_hidden , dW_out , db_out)=gradients\n",
        "          (kernel1_old , kernel2_old , W_hidden_old , b_hidden_old, W_out_old , b_out_old )=parameters\n",
        "          (V_kernel1 , V_kernel2, V_W_hidden , V_b_hidden , V_W_out , V_b_out) =V\n",
        "          \n",
        "          V_kernel1= alpha*V_kernel1 - learn_rate *(dkernel1)\n",
        "          kernel1_new=kernel1_old + V_kernel1\n",
        "          V_kernel2 = alpha*V_kernel2 - learn_rate* (dkernel2) \n",
        "          kernel2_new = kernel2_old + V_kernel2\n",
        "      \n",
        "          V_W_hidden = alpha*V_W_hidden - learn_rate* (dW_hidden) \n",
        "          W_hidden_new = W_hidden_old + V_W_hidden\n",
        "          V_b_hidden = alpha*V_b_hidden - learn_rate* (db_hidden) \n",
        "          b_hidden_new = b_hidden_old + V_b_hidden\n",
        "          \n",
        "          V_W_out = alpha*V_W_out - learn_rate* (dW_out) \n",
        "          W_out_new = W_out_old + V_W_out\n",
        "          V_b_out = alpha*V_b_out - learn_rate* (db_out) \n",
        "          b_out_new = b_out_old + V_b_out\n",
        "\n",
        "          parameters=(kernel1_new ,kernel2_new ,W_hidden_new ,b_hidden_new , W_out_new , b_out_new)\n",
        "          V_new=(V_kernel1 , V_kernel2, V_W_hidden , V_b_hidden , V_W_out , V_b_out) \n",
        "          return parameters,V_new\n",
        "                "
      ],
      "metadata": {
        "id": "22ESAN8HLahr"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSprop(gradients, parameters ,V):\n",
        "          learn_rate=0.001\n",
        "          beta = 0.9\n",
        "          epsilon= 1e-07\n",
        "          (dkernel1 , dkernel2, dW_hidden , db_hidden , dW_out , db_out)=gradients\n",
        "          (kernel1_old , kernel2_old , W_hidden_old , b_hidden_old, W_out_old , b_out_old )=parameters\n",
        "          (V_kernel1 , V_kernel2, V_W_hidden , V_b_hidden , V_W_out , V_b_out) =V\n",
        "          \n",
        "          V_kernel1= beta*V_kernel1 + (1-beta)*(dkernel1**2)\n",
        "          V_kernel2 = beta*V_kernel2 + (1-beta)* (dkernel2**2) \n",
        "          V_W_hidden = beta*V_W_hidden + (1-beta)* (dW_hidden**2) \n",
        "          V_b_hidden = beta*V_b_hidden + (1-beta)* (db_hidden**2)         \n",
        "          V_W_out = beta*V_W_out + (1-beta)* (dW_out**2) \n",
        "          V_b_out = beta*V_b_out + (1-beta)* (db_out**2) \n",
        "       \n",
        "          kernel1_new = kernel1_old - learn_rate* (dkernel1 /np.sqrt(V_kernel1 + epsilon) ) \n",
        "          kernel2_new = kernel2_old - learn_rate* (dkernel2 /np.sqrt(V_kernel2+ epsilon)) \n",
        "          W_hidden_new = W_hidden_old - learn_rate* (dW_hidden / np.sqrt(V_W_hidden + epsilon)) \n",
        "          b_hidden_new = b_hidden_old - learn_rate* (db_hidden / np.sqrt(V_b_hidden + epsilon)) \n",
        "          W_out_new = W_out_old - learn_rate* (dW_out / np.sqrt(V_W_out + epsilon)) \n",
        "          b_out_new = b_out_old - learn_rate* (db_out / np.sqrt(V_b_out+ epsilon)) \n",
        "\n",
        "          parameters=(kernel1_new ,kernel2_new ,W_hidden_new ,b_hidden_new , W_out_new , b_out_new)\n",
        "          V_new=(V_kernel1 , V_kernel2, V_W_hidden , V_b_hidden , V_W_out , V_b_out) \n",
        "          return parameters,V_new"
      ],
      "metadata": {
        "id": "hbdCUDzfbQrI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross Entropy Loss\n",
        "def cross_entropy_god(targets,predictions,  epsilon=1e-19):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between targets (encoded as one-hot vectors) and predictions. \n",
        "    Input: predictions (Num_datapoints, num_classlabels) ndarray\n",
        "           targets (Num_datapoints, num_classlabels) ndarray        \n",
        "    Returns: scalar\n",
        "    \"\"\"\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    N = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
        "    return ce"
      ],
      "metadata": {
        "id": "psIvIfHp5tIL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only forward prpogation\n",
        "def forward_path(data_points , current_parameters):\n",
        "      (kernel1 ,kernel2 , weight_hidden ,bias_hidden, weight_out , bias_out) =  current_parameters   \n",
        "      # forward propagation\n",
        "      conv1 , bla = conv_forward(data_points, kernel1)\n",
        "      maxpool1 = pool_forward(conv1)\n",
        "                \n",
        "      conv2 , bla = conv_forward(maxpool1 , kernel2 )\n",
        "      maxpool2 = pool_forward( conv2)\n",
        "                \n",
        "      flatten_feature = flat_forward(maxpool2)\n",
        "      prediction, bla = MLP_forward(flatten_feature , weight_hidden ,bias_hidden , weight_out ,bias_out)\n",
        "      \n",
        "      return  prediction"
      ],
      "metadata": {
        "id": "tG5fzrOjn8HR"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tsne_plotting (feature_vec ,labels , string):\n",
        "       colorss=['blue', 'black', 'red', 'green', 'yellow', 'cyan','orange','brown','violet','grey']\n",
        "       z = TSNE(n_components=2, verbose=1, random_state=123).fit_transform(feature_vec)\n",
        "       class_co = np.array( tf.argmax(labels, axis=1) )\n",
        "       plt.figure()\n",
        "       for i in range(labels.shape[0]):\n",
        "          plt.plot(z[i][0], z[i][1], marker=\"o\", markersize=5, color= colorss[class_co[i]])\n",
        "       \n",
        "       plt.title(\"t-sne plot at the end of epoch = \"+ string)\n",
        "       plt.show()\n"
      ],
      "metadata": {
        "id": "PNYYpslRWpnH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(data_labels, num_classes):\n",
        "        #Encode labels into a one-hot representation\n",
        "        onehot = np.zeros((data_labels.shape[0],num_classes))\n",
        "        for i in range(data_labels.shape[0]):\n",
        "            onehot[i,data_labels[i]] = 1.0\n",
        "        return onehot"
      ],
      "metadata": {
        "id": "BHij6XlPafCw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Complete_model(SGD_variant ,train_images , train_labels,num_epoch=15 ):\n",
        "       \n",
        "       #Initialisation:\n",
        "       num_input_MLP=(196)\n",
        "       num_hidden_MLP=49\n",
        "       num_out_MLP=10\n",
        "       parameter_SGD_variant=(0,0,0,0,0,0)   \n",
        "\n",
        "       kernel1= np.random.randn(5, 5, 1, 4)\n",
        "       kernel2= np.random.randn(5, 5, 4, 4)\n",
        "       weight_hidden= np.random.rand(num_input_MLP,num_hidden_MLP)\n",
        "       bias_hidden = np.random.randn(num_hidden_MLP)\n",
        "       weight_out = np.random.rand(num_hidden_MLP,num_out_MLP)\n",
        "       bias_out = np.random.randn(num_out_MLP)\n",
        "       \n",
        "       train_error=  [0] * (num_epoch)  # stores the cross entropy train error at the end of each epoch\n",
        "       test_error =  [0] * (num_epoch)   # stores the cross entropy test error at the end of each epoch\n",
        "       accuracy =  [0] * (num_epoch)  # stores the validate accuracy at the end of each epoch\n",
        "\n",
        "       # forward propagation\n",
        "       conv1 , info_convback1 = conv_forward(train_images , kernel1)\n",
        "       maxpool1 = pool_forward(conv1)\n",
        "                \n",
        "       conv2 , info_convback2 = conv_forward(maxpool1 , kernel2 )\n",
        "       maxpool2 = pool_forward( conv2)\n",
        "                \n",
        "       flatten_feature = flat_forward(maxpool2)\n",
        "       y_train_prediction,info_MLP_back = MLP_forward(flatten_feature , weight_hidden ,bias_hidden , weight_out ,bias_out)\n",
        "\n",
        "       for ep in range(num_epoch ):\n",
        "\n",
        "                #backward propagation\n",
        "                grad_flatten,grad_Wout,grad_bout,grad_Whid,grad_bhid = MLP_backward(train_labels,info_MLP_back)\n",
        "                grad_pool2 = flat_backpropagation(grad_flatten, maxpool2)\n",
        "                \n",
        "                grad_conv2 = pool_backward(grad_pool2 , conv2)\n",
        "                grad_pool1, grad_ker2 =conv_backward(grad_conv2 , info_convback2)\n",
        "\n",
        "                grad_conv1 =pool_backward(grad_pool1 , conv1)\n",
        "                d_input , grad_ker1 =conv_backward(grad_conv1 , info_convback1)\n",
        "\n",
        "                # current parameters\n",
        "                all_gradient = (grad_ker1 ,grad_ker2 ,grad_Whid ,grad_bhid ,grad_Wout ,grad_bout )\n",
        "                all_parameter = (kernel1 ,kernel2 , weight_hidden ,bias_hidden, weight_out , bias_out)\n",
        "\n",
        "                \n",
        "                #weight updation\n",
        "                if (SGD_variant == \"Vanilla_SGD\"):\n",
        "                    all_parameter_updated = Vanilla_SGD(all_gradient , all_parameter) \n",
        "                elif(SGD_variant == \"Momentum\"):\n",
        "                    all_parameter_updated , parameter_SGD_variant =Momentum(all_gradient ,all_parameter ,parameter_SGD_variant)\n",
        "                elif(SGD_variant == \"RMSprop\"):\n",
        "                    all_parameter_updated , parameter_SGD_variant =RMSprop(all_gradient ,all_parameter ,parameter_SGD_variant)     \n",
        "\n",
        "                (kernel1 ,kernel2 , weight_hidden ,bias_hidden, weight_out , bias_out)=all_parameter_updated     \n",
        "                \n",
        "                # forward propagation\n",
        "                conv1 , info_convback1 = conv_forward(train_images , kernel1)\n",
        "                maxpool1 = pool_forward(conv1)\n",
        "                \n",
        "                conv2 , info_convback2 = conv_forward(maxpool1 , kernel2 )\n",
        "                maxpool2 = pool_forward( conv2)\n",
        "                \n",
        "                flatten_feature = flat_forward(maxpool2)\n",
        "                y_train_prediction,info_MLP_back = MLP_forward(flatten_feature , weight_hidden ,bias_hidden , weight_out ,bias_out)\n",
        "                \n",
        "                train_error[ep]= cross_entropy_god(train_labels , y_train_prediction)\n",
        "                \n",
        "                \n",
        "                 # Shuffle the training data\n",
        "                \n",
        "                permutation = np.random.permutation(len(train_images))\n",
        "                train_images = train_images[permutation]\n",
        "                train_labels = train_labels[permutation] \n",
        "                \n",
        "               \n",
        "       #printing the requied   \n",
        "       epoch_list=np.array(range(1,num_epoch+1))\n",
        "       plt.figure() \n",
        "       plt.plot( epoch_list,train_error )\n",
        "       plt.xlabel('number of epochs completed')\n",
        "       plt.ylabel('train error')\n",
        "       plt.title('train error with each epoch')\n",
        "       plt.show()\n",
        "\n",
        "       return all_parameter_updated     "
      ],
      "metadata": {
        "id": "UzKqmG5l7ITS"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(train_images_data, train_labels_data), (test_images_data, test_labels_data) = tf.keras.datasets.mnist.load_data()\n"
      ],
      "metadata": {
        "id": "RjaWsthxX211"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 100 images per class for the training set i.e., use 1000 images for training."
      ],
      "metadata": {
        "id": "9AgJP-_ftGpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dictionary ={i:[] for i in range(10)}\n",
        "for img,label in zip(train_images_data,train_labels_data):\n",
        "  train_dictionary[label].append(np.array(img).reshape(28,28,1))\n",
        "\n",
        "traindata=[]\n",
        "trainlab=[]\n",
        "for i in range(10):\n",
        "  traindata.append(np.array(train_dictionary[i][:100]))\n",
        "  trainlab.append([i]*100)\n",
        "\n",
        "train_images=np.array(traindata).reshape(1000,28,28,1)  \n",
        "train_labels=np.array(trainlab).reshape(1000,1)"
      ],
      "metadata": {
        "id": "clIDI54XWh1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dictionary ={i:[] for i in range(10)}\n",
        "for img,label in zip(test_images_data,test_labels_data):\n",
        "  test_dictionary[label].append(np.array(img).reshape(28,28,1))\n",
        "\n",
        "testdata=[]\n",
        "testlab=[]\n",
        "for i in range(10):\n",
        "  testdata.append(np.array(test_dictionary[i][:100]))\n",
        "  testlab.append([i]*100)\n",
        "\n",
        "test_images=np.array(testdata).reshape(1000,28,28,1)  \n",
        "test_labels=np.array(testlab).reshape(1000,1)"
      ],
      "metadata": {
        "id": "x9XVPZ0aXVfD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels=encode_labels(train_labels,10)\n",
        "test_labels=encode_labels(test_labels,10)"
      ],
      "metadata": {
        "id": "Xt_hY81wbmcC"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_copy= train_images\n",
        "train_labels_copy= train_labels\n",
        "\n",
        "all_parameters=Complete_model( \"Momentum\" , train_images_copy ,train_labels_copy ,20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "HKUUn0-QGYrA",
        "outputId": "f6890fdb-9efd-441b-a13e-37d9cef20723"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images_prediction= forward_path(test_images,all_parameters) "
      ],
      "metadata": {
        "id": "d7KtwGixJcAP"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_data_point=test_images[69].reshape(28,28)\n",
        "plt.imshow(img_data_point)\n",
        "print(\"Actual label=\", (test_labels[69]))\n",
        "print(\"model predicted label=\",max_mask(test_images_prediction[69]) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "2BFITfExKK0B",
        "outputId": "279410fa-d843-44fd-ac94-235b6b491d35"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual label= [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "model predicted label= [ True False False False False False False False False False]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOrklEQVR4nO3df4wc9XnH8c9zl/M5/oHks8FxHVQScN24aeLgi42KldCSuA40NVSNFTdyjIp0VAXVifjDhFYGVa1k2gKiqgEdwcKhlAgSKJZCSmyLyIVS4wNc/ySYWnZt1/hCLef8A/+8p3/cgA64+e6xO7uz5+f9kk67N8/OzsPiz83ufmfma+4uAOe/lrIbANAYhB0IgrADQRB2IAjCDgTxsUZubJS1+2iNbeQmgVBO6rhO+ykbqlZT2M1svqT7JLVK+r67r0g9frTGao5dXcsmASRs9PW5tarfxptZq6SVkr4maYakRWY2o9rnA1BftXxmny3pTXff7e6nJf1Q0oJi2gJQtFrCPlXSvkG/78+WvY+ZdZlZj5n1nNGpGjYHoBZ1/zbe3bvdvdPdO9vUXu/NAchRS9gPSLp40O+fzJYBaEK1hH2TpGlm9ikzGyXpm5LWFNMWgKJVPfTm7mfN7BZJz2lg6G2Vu28vrDMAhappnN3dn5X0bEG9AKgjDpcFgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IIiGTtmM80/rpInJ+rG5l+bW9s1PP7edSu+Lpi39z/QT4H3YswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzI+nMV2Yl68u7H0rWr2hfW/W23/HTyfqXd3w3WZ+84e3c2rmdu6rqaSSrKexmtkfSUUnnJJ11984imgJQvCL27L/r7vl/QgE0BT6zA0HUGnaX9DMze8XMuoZ6gJl1mVmPmfWc0akaNwegWrW+jZ/r7gfM7CJJa83sdXffMPgB7t4tqVuSLrAOr3F7AKpU057d3Q9kt72SnpY0u4imABSv6rCb2VgzG//ufUnzJG0rqjEAxarlbfxkSU+b2bvP8y/u/m+FdIWGOTMvPVq6/IFVyfoV7dVv+2/e/myyfvukrcn6y8tXJuuP9P1abu3Jb381ua5vSm97JKo67O6+W9LnC+wFQB0x9AYEQdiBIAg7EARhB4Ig7EAQ5t64g9ousA6fY1c3bHthtLTmlt64P32K6up53cn6le39yfqtb6WPo3rhwS/m1i589LXkun1/ODNZX/a3P0jWrx1zLLf2+pn0odu3fmPIo7/f06xDcxt9vfr8sA1VY88OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0FwKenzwBsP549HvznvwZqe+/JN30rWp97wv8n6xCMv5dbSI/jSuCfSUzKv/J8/Ttb7H30qt/b1MX3JdXf9RToaly1OlpsSe3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCILz2UeA/rnp87qfePz+3NoFLaOT66YutyxJP5o9LVnvP3o0WS/TOwvyz7V//v708Qe9504k6zfO/9Nk/dyON5L1euF8dgCEHYiCsANBEHYgCMIOBEHYgSAIOxAE57M3Af+d9GS4Kx5NX9s9NZb+5a3pc77Hf+OXyXozj6NX0n7kTNXrXtQ6Jln3tvxr9Terint2M1tlZr1mtm3Qsg4zW2tmu7LbCfVtE0CthvM2/hFJ8z+w7DZJ6919mqT12e8AmljFsLv7BkmHP7B4gaTV2f3Vkq4ruC8ABav2M/tkdz+Y3X9L0uS8B5pZl6QuSRqt9OcgAPVT87fxPnAmTe7ZNO7e7e6d7t7ZpvZaNwegStWG/ZCZTZGk7La3uJYA1EO1YV8jaUl2f4mkZ4ppB0C9VPzMbmaPS7pK0iQz2y/pDkkrJD1hZjdK2itpYT2bPN/tXZq+psDMUdUfDnHk+U8k62OP7q76uTGyVPxX5O6LckpchQIYQThcFgiCsANBEHYgCMIOBEHYgSA4xbUBTvzRnGT9yTn3VXiGUcnqz0+25dam3vUfFZ77/HXiovTrFg17diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2ArRO7EjWf++OF5L132qrbTx4xQ3fzq216LWannske2fxkarX3Xz6bLJuJ05V/dxlYc8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzl6Ak7M+nawvn7Supuf/UqVpl1/amltLX6R6ZDv5B7OT9Qd/e2Wimp5y+c/+emmy3rHrpWS9GbFnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGcvwL4b0uc+V/LksYnJ+vjvfTxZ97O1bX+k2rcw/d89a1R6LD1l/L7TVa/brCru2c1slZn1mtm2QcvuNLMDZrY5+7mmvm0CqNVw3sY/Imn+EMvvdfeZ2c+zxbYFoGgVw+7uGyQdbkAvAOqoli/objGzLdnb/Al5DzKzLjPrMbOeMxp51+0CzhfVhv0BSZdKminpoKS78x7o7t3u3ununW1qr3JzAGpVVdjd/ZC7n3P3fkkPSUqffgSgdFWF3cymDPr1eknb8h4LoDlUHGc3s8clXSVpkpntl3SHpKvMbKYGTpfeI+mmOvbY9C7q6Ktp/bte//3087+2vabnH6lOXJ+e1/7Fq3I/PWbG5FYu+2lXcs3p/55/jQBpZF4noGLY3X3REIsfrkMvAOqIw2WBIAg7EARhB4Ig7EAQhB0IglNch6sl/3TJUa3nGtjIyGLt+UdNHr92ZnLdn/7jfcn6uJZxyfqyQ/nP/xtd6amsvf/8+3/Knh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcfZhaPjc9t7Zuxj83sJORZe+yWbm1bTf9U3LdVhudrM96ZWGyfuHytvxif7zThtmzA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLMjqeVzv5msv37z+GT9ufl/n6imp6L+1+Pp89WT4+iSPOgluPOwZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnHybbvT+39ucHrkyue//UF5P16ZN6k/VfjR2brPcfP55baxmTP22xJPUu/nyyvnJZ+pzzL7Zbsp4aS//JifQ4+oMLFyTrjKN/NBX37GZ2sZk9b2Y7zGy7mS3NlneY2Voz25XdTqh/uwCqNZy38Wcl3eruMyRdIelmM5sh6TZJ6919mqT12e8AmlTFsLv7QXd/Nbt/VNJOSVMlLZC0OnvYaknX1atJALX7SJ/ZzewSSV+QtFHSZHc/mJXekjQ5Z50uSV2SNFrpz48A6mfY38ab2ThJP5b0HXfvG1xzd5fkQ63n7t3u3ununW3Kn+QPQH0NK+xm1qaBoD/m7k9liw+Z2ZSsPkVS+itlAKWq+DbezEzSw5J2uvs9g0prJC2RtCK7faYuHTaJc319ubW1L89Jr3x9eujtsUvWJet3vfSZZP1X5/KHt8a3Hk6u+72JK5N1qdLQWtplP7kpt/aZe48k1/WdDK0VaTif2a+UtFjSVjPbnC27XQMhf8LMbpS0V1L6It4ASlUx7O7+gvL/vF9dbDsA6oXDZYEgCDsQBGEHgiDsQBCEHQjCBg5+a4wLrMPn2Pn3BX5LhVNQp284lazf/YmXi2ynUC+eSu8P/uq7Xcn62Oe25Nb6T56sqifk2+jr1eeHhxw9Y88OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0FwKekCpC7lLEnbl85M1i/7k8uT9XXX3pOsX/Kx/Mt9fWvPV5Lrvvbz6cn6pff8Iln/+P+ljxHoT1bRSOzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIzmcHziOczw6AsANREHYgCMIOBEHYgSAIOxAEYQeCqBh2M7vYzJ43sx1mtt3MlmbL7zSzA2a2Ofu5pv7tAqjWcC5ecVbSre7+qpmNl/SKma3Nave6+z/Urz0ARRnO/OwHJR3M7h81s52Spta7MQDF+kif2c3sEklfkLQxW3SLmW0xs1VmNiFnnS4z6zGznjNKT4MEoH6GHXYzGyfpx5K+4+59kh6QdKmkmRrY89891Hru3u3une7e2ab2AloGUI1hhd3M2jQQ9Mfc/SlJcvdD7n7O3fslPSRpdv3aBFCr4Xwbb5IelrTT3e8ZtHzKoIddL2lb8e0BKMpwvo2/UtJiSVvNbHO27HZJi8xspiSXtEfSTXXpEEAhhvNt/AuShjo/9tni2wFQLxxBBwRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCKKhUzab2S8l7R20aJKktxvWwEfTrL01a18SvVWryN5+3d0vHKrQ0LB/aONmPe7eWVoDCc3aW7P2JdFbtRrVG2/jgSAIOxBE2WHvLnn7Kc3aW7P2JdFbtRrSW6mf2QE0Ttl7dgANQtiBIEoJu5nNN7NfmNmbZnZbGT3kMbM9ZrY1m4a6p+ReVplZr5ltG7Ssw8zWmtmu7HbIOfZK6q0ppvFOTDNe6mtX9vTnDf/Mbmatkt6Q9FVJ+yVtkrTI3Xc0tJEcZrZHUqe7l34Ahpl9SdIxST9w989my/5O0mF3X5H9oZzg7suapLc7JR0rexrvbLaiKYOnGZd0naQbVOJrl+hroRrwupWxZ58t6U133+3upyX9UNKCEvpoeu6+QdLhDyxeIGl1dn+1Bv6xNFxOb03B3Q+6+6vZ/aOS3p1mvNTXLtFXQ5QR9qmS9g36fb+aa753l/QzM3vFzLrKbmYIk939YHb/LUmTy2xmCBWn8W6kD0wz3jSvXTXTn9eKL+g+bK67Xy7pa5Juzt6uNiUf+AzWTGOnw5rGu1GGmGb8PWW+dtVOf16rMsJ+QNLFg37/ZLasKbj7gey2V9LTar6pqA+9O4Nudttbcj/vaaZpvIeaZlxN8NqVOf15GWHfJGmamX3KzEZJ+qakNSX08SFmNjb74kRmNlbSPDXfVNRrJC3J7i+R9EyJvbxPs0zjnTfNuEp+7Uqf/tzdG/4j6RoNfCP/35L+sowecvr6tKT/yn62l92bpMc18LbujAa+27hR0kRJ6yXtkrROUkcT9faopK2StmggWFNK6m2uBt6ib5G0Ofu5puzXLtFXQ143DpcFguALOiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0I4v8BPq1fahgchGMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}